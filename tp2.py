# -*- coding: utf-8 -*-
"""TP2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SIO_HoZFAnQWeRNEWKe7a1WlrIeV6ZfA
"""

import numpy as np
import cv2 as cv 
from matplotlib import pyplot as plt
from matplotlib.widgets import Slider
from numpy.fft import fft2, ifft2, fftshift
import random
def fourier():
  x, y = np.meshgrid(range(0, 256), range(0, 256)) # création des images

  A = random.randint(0,9)
  B = random.randint(0,9)
  I = 128*(np.sin(2*np.pi*(A*x + B*y)) + 1)
  return I;
def ima():
  x, y = np.meshgrid(range(0, 256), range(0, 256)) # création des images
  A = random.randint(0,9)
  B = random.randint(0,9)
  i = 128*np.sin(2*np.pi*(A*x + B*y))
  #fig = plt.figure(figsize=(10,7))
  #rows = 1
  #colums = 2
  #fi1 = fft2(i)
  #m_fi1 = abs(fi1)
  #m_fi1 = fftshift(m_fi1) 
 # fig.add_subplot(rows,colums,1)
 # plt.imshow(m_fi1,cmap=plt.get_cmap('gray'))
  #plt.title('Image des frequences de i')
  return i;



def main ():
  
  
  rows = 1
  colums = 2

 
  photofore = cv.imread('photophore(1).tif', 0)
  plt.imshow(photofore, cmap='gray')
  plt.show()


  fig = plt.figure(figsize=(10,7))
  fig.add_subplot(rows,colums,1)
  #plt.imshow(photofore,cmap = plt.get_cmap('gray'))
  plt.title('Image I pertubé')
  #fig = plt.figure(figsize=(10,7))
  #fig.add_subplot(rows,colums,1)
  #plt.imshow(photofore,cmap=plt.get_cmap('gray'))
  #plt.title('Image de I ')
  

if __name__ == "__main__":
  main()

import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt

# Méthode du masque
def contour_laplacien(im_src,seuil):
  # Création du masque

  flt1 = np.zeros((3,3))
  flt1[0][1] = 1
  flt1[1][0] = 1
  flt1[1][1] = -4
  flt1[1][2] = 1
  flt1[2][1] = 1

  # Laplacien de l'image  
  imlap = cv.filter2D(im_src, -1 , flt1 )
  #plt.imshow(imlap,cmap="gray")
  # Image de polarité booléenne
  
  impol = imlap > 0
  #plt.imshow(impol,cmap="gray")
  #pos = im > 20
  # Image des passages par zéro
  impol0 = impol[:-1,:-1]
  impol1 = impol[1:,:-1]
  impol2 = impol[:-1,1:]
  imzero = np.logical_or(impol0!= impol1 , impol0 != impol2).astype(int)
  #plt.imshow(imzero,cmap="gray")
  # Affichage
  gx = np.array([ [1,0,-1], [2,0,-2], [1,0,-1]])
  gy = gx.transpose()
  gradient = cv.filter2D(im_src, -1 , gx , borderType=cv.BORDER_CONSTANT).astype(int)**2+\
  cv.filter2D(im_src, -1 , gy , borderType=cv.BORDER_CONSTANT).astype(int)**2
  gradient = gradient[:-1,:-1]
  result = np.logical_and(imzero,(gradient > seuil))
  #plt.imshow(result,cmap="gray")
  return result;

im = cv.imread('photophore.tif', cv.IMREAD_GRAYSCALE)

seuil = 1600
seuil_haut = 140
seuil_bas = 0.7 * seuil_haut

refcanny = cv.Canny(im , seuil_haut , seuil_bas)
reflapla  = contour_laplacien(im , seuil)

#plt.imshow(reflapla,cmap="gray")

CannyTauxErreur = [];
LaPlacienTauxErruer = [];

for i in (5,10,15,20):
  #image bruité
  imBruit = im + i*np.random.randn(im.shape[0],im.shape[1]).astype('uint8')

  #taux d'erreur de canny
  refcannyBruit = cv.Canny(imBruit,seuil_haut,seuil_bas);
  difCanny = (refcanny != refcannyBruit);
  tauxCanny =difCanny.sum();
  CannyTauxErreur.append(tauxCanny)

  # taux d'erreur de la place
  refLaplacienBruit = contour_laplacien(imBruit,seuil)
  difLaplacien = (reflapla != refLaplacienBruit)
  tauxLaplacien = difLaplacien.sum();
  LaPlacienTauxErruer.append(tauxLaplacien)

# Affichage taux d'erreurs des deux algorithmes.

print("Taux d'erreur Canny : " +str(CannyTauxErreur))
print("Taux d'erreur Laplacien : "+str(LaPlacienTauxErruer))


# Calculs des taux d'erreurs pour Canny
#for i in range(5,25,5):
  #im2 = (im + i*np.random.randn(im.shape[0],im.shape[1])).astype('uint8')
  #flt2 = np.array([ [0,1,0],[1,-4,1],[0,1,0]])
  #contour_l = cv.filter2D(im2, -1 ,flt2, borderType= cv.BORDER_CONSTANT)
  #diflaplace = np.not_equal(reflapla, contour_l[:-1,:-1])
  #plt.imshow(diflaplace)
  #tauxlaplace = diflaplace.sum()
  #print(tauxlaplace)
  #canny = im + i*np.random.randn(im.shape[0],im.shape[1]).astype('uint8')
  #diftauxCanny = refcanny != canny
  #plt.imshow(diftauxCanny)
  #tauxCanny = diftauxCanny.sum()
  #print(tauxCanny)

import numpy as np
import cv2 as cv 
from matplotlib import pyplot as plt
from matplotlib.widgets import Slider
from numpy.fft import fft2, ifft2, fftshift
import random

photofore = cv.imread('photophore.tif', 0)
#plt.imshow(photofore)
plt.show()

path='photophore.tif'
img = cv.imread(path)

plt.imshow(img)
plt.show()

"""# Nouvelle section

# Nouvelle section

# Nouvelle section
"""

I = fourier()
  print(I)
  
  
  # conclusion : à chaque couple A et B correspond une image I.
  # les pixels de l'image I varient en fonction des valeurs de variable A et B

  #tranformé de fourier de l'image I
  #fi = fft2(I)
  #m_fi = abs(fi)
 # m_fi = fftshift(m_fi)
  #fig.add_subplot(rows,colums,2)
  #plt.imshow(m_fi,cmap=plt.get_cmap('gray'))
  #plt.title('Image des fréquences')
  #Conclusion: On n'observe presque pas de changement  
  I1 = fourier()
  I2 = fourier()
  I3 = fourier()
  
 # plt.imshow(I1,cmap=plt.get_cmap('gray'))
  #plt.title('Image I1')
  #fig.add_subplot(rows,colums,1)
  #plt.imshow(I2,cmap=plt.get_cmap('gray'))
  #plt.title('Image I2')

  i = I1 + I2
  i3 = I1 + I2 + I3
  fi1 = fft2(i)
  m_fi1 = abs(fi1)
  m_fi1 = fftshift(m_fi1) 
  #fig.add_subplot(rows,colums,2)
  #plt.imshow(m_fi1,cmap=plt.get_cmap('gray'))
  #plt.title('Image des frequences de i')
  #i = 0
  #print(module_trans_fourier(i3,fig,rows,colums))




fig = plt.figure(figsize=(10,7))
  fig.add_subplot(rows,colums,1)
  plt.imshow(I,cmap=plt.get_cmap('gray'))
  plt.title('Image I')

def module_trans_fourier(i3,fig,rows,colums):
  fi1 = fft2(i3)
  m_fi1 = abs(fi1)
  m_fi1 = fftshift(m_fi1) 
  fig.add_subplot(rows,colums,2)
  plt.imshow(m_fi1,cmap=plt.get_cmap('gray'))
  plt.title('frequences de plus de 2 images')








import numpy as np
import matplotlib.pyplot as plt

def convol():
  vet = np.random.rand(16 , 1024)
  psy = np.random.rand(16)
  dist = np.array([np.linalg.norm(vet[tmp] - vet[tmp1]) for tmp in range(len(vet)) for tmp1 in range(len(vet)) if tmp != tmp1])
  plt.hist(dist , label="distance")
    
  convol = np.array([np.convolve(psy ,vecteur) for vecteur in vet ])
  seuil = 0.2* np.median(convol)
  convol[convol >= seuil] = 1
  convol[convol < seuil] = 0
  dist_convol = np.array([np.linalg.norm(convol[i] - convol[j]) for i in range(len(convol)) for j in range(len(convol)) if i != j])
  plt.hist(dist_convol , label="distance convol")
  plt.legend()
  plt.xlabel("value")
  plt.ylabel("nombre")
  plt.show()


    
 
 
  

def main ():
  
   print(convol());
  

if __name__ == "__main__":
  main()

import math
def modular_pow(base,exponent, modulus): 
	result = 1
	base = base%modulus
	while (exponent > 0):
		if(exponent % 2 == 1):
			result = (result * base) % modulus
		exponent = exponent >> 1
		base = (base*base)% modulus
	return float(result);

def expo_modulaire(a,b,p):
  d = pow(a, b, p)
  return d;
def main ():
 a = 123
 b = 7
 p = 179
 print(expo_modulaire(a,b,p));
 print(modular_pow(a,b,p))
 

if __name__ == "__main__":
  main()

#1.2 Inversion modulaire
def find_mod_inv(a,m):
    for x in range(1,m):
        if((pgcd(a,m))*(pgcd(x,m)) % m==1):
            return x
def pgcd(m,n): 
    while m%n: 
        r=m%n 
        m=n 
        n=r 
    return n 
 
def PremiersEntreEux(m,n): 
  r0 = n
  r1 = m%n
  print(str(m) + " = " + str(r0) + " * " + str(m//n) + " + " + str(r1))
  while(r1 != 0):
      x = r1
      r1 = r0%r1
      print(str(r0) + " = " + str(x) + " * " + str(r0//x) + " + " + str(r1))
      r0 = x
  print("Le PGCD de a et b est: " + str(r0))
  if(pgcd(m,n)==1):
    x = 1;
  else:
    x = "choisi d'autre valeur car ils ne sont pas prémiers entre eux."; 
  return x;
def bezout_fct(a,b):
    if b == 0:
        return 1,0
    else:
        u , v = bezout_fct(b , a % b)
        return v , u - (a//b)*v

def resoud_equation(a,b,c):
    u,v = bezout_fct(a,b)
    return "Les solutions de l'équation {}x + {}y = {} sont:\n({} + {}k , {} - {}k)".format(a,b,c,u,b,v,a)

def inv_modulo(a, b):
    ''' Calcule y dans [[0, b-1]] tel que x*y % abs(b) = 1 '''
    (u,p) = bezout_fct(a, b)
    if p == 1: z = u%abs(b)
    else: 
      z = "ils ne sont pas premiers entre eux";
      print(z);
      

    return z;
def main ():
  print("!!==PGCD de a et b==!!")
  a = int(input('a = '))
  b = int(input('b = '))
  print(PremiersEntreEux(a,b)) 
  c = PremiersEntreEux(a,b);
  #if(c == 1):
  print(bezout_fct(a,b));
  print(resoud_equation(a,b,c));
  res = inv_modulo(a,b);
  print("l'inverse modulo est: "+res);
  #print("L'inverse de "+"%s"+" modulo "+"%s"+" est: "+ "% (a,b,inv_modulo(a,b)");
  #print "L'inverse de %s modulo %s est : %s" % (x,m,inv_modulo(x,m))
  print(pgcd(a,b))
  

if __name__ == "__main__":
  main()

"""# Nouvelle section"""

# 1.3 Chiffrement RSA
def primesInRangeFermat(x, y):
    prime_list = []
    for n in range(x, y):
        isPrime = True

        for num in range(2, n):
            if n % num == 0:
                isPrime = False
        if isPrime:
            prime_list.append(n)           
    return prime_list
def find_mod_inv(a,m):

    for x in range(1,m):
        if((a%m)*(x%m) % m==1):
            return x  

def prime_generator(end):
    for n in range(2, end):     
        for x in range(2, n):   
            if n % x == 0:      
                break
        else:                   
            yield n                                
def main ():
  n = 989
  k = 29
  a = 534
  chiffrer = pow(a, k, n)
  print(chiffrer)
  clés =find_mod_inv(k,n)
  déchiffrer = pow(chiffrer, clés, n)
  print(clés)
  print(déchiffrer)
  print(primesInRangeFermat(10000, 10010))
  g = prime_generator(1000)
  print(list(g))

if __name__ == "__main__":
  main()



from random import randrange

def PGCD(a, b):
    """Retourne le PGCD de a et b."""
    if b == 0:
        return a
    else:
        return PGCD(b, a%b)

def modulo(a, b, m):
    """Retourne (a**b) % m"""
    res = 1
    while b > 0:
        if b%2 == 1:
            res = (res * a) % m
            b -= 1
        b /= 2
        a = (a**2) % m
    return res

def temoinMiller(a, n, s, d):
    """Retourne True si a est un témoin de Miller, False sinon."""
    x = modulo(a, d, n)
    if x == 1 or x == n-1:
        return False
    while s > 1:
        x = modulo(x, 2, n)
        if x == n-1:
            return False
        s -= 1
    return True

def millerRabin(n, k = 25):
    """Retourne False si n est composé, True si n est probablement premier."""
    s = 0
    while (n-1)/2**(s+1) == int((n-1)/2**(s+1)):
        s += 1
    d = int(n/2**s)
    for i in range(k):
        a = randrange(2, n-1)
        if temoinMiller(a, n, s, d):
            return False
    return True

def bezout(a, b):
    """Retourne [u, v] tels que au + bv = 1."""
    coefficients = [[1, 0], [0, 1]]
    while a%b != 0:
        coefficients.append([coefficients[-2][0] - (a//b) * coefficients[-1][0], coefficients[-2][1] - (a//b) * coefficients[-1][1]])
        a, b = b, a%b
    return (coefficients[-1])

def inverse_modulaire(a, m):
    """Retourne x tel que ax % m = 1."""
    return (bezout(a, m)[0] % m)

#Détermination de deux nombres premiers p et q.
premiers = []
while len(premiers) != 2:
    a = randrange(10**9, 10**10)
    if a%2 != 0 and millerRabin(a, 25):
        premiers.append(a)

p, q = premiers[0], premiers[1]

print(p, q)

n = p * q

phi = (p-1) * (q-1)

#Détermination d'un entier e tel que PGCD(e, phi) = 1.
e = randrange(int(phi/10), phi)
while PGCD(e, phi) != 1:
    e = randrange(int(phi/10), phi)

#Détermination de d tel que e*d % phi = 1.
d = inverse_modulaire(e, phi)

print("Clé publique: ({}, {}) \nClé privée: ({}, {})".format(n, e, d, n))

#Message à coder.
m = 12

#c = (m**e) % n message codé.
c = modulo(m, e, n)

#m2 = (c**d) % n message décodé.
m2 = modulo(c, d, n)

print(m, m2)

import numpy as np
from scipy import signal
from scipy.linalg import norm
from numpy import linalg as LA
import matplotlib.pyplot as plt
def calc_Lp(x1 , x2, q=2):
  dim = x1.shape[0]
  i = 0
  sum = 0
  while(i < dim):
    sum+= abs(x1[i] - x2[i])**q
    i+=1
  return (sum**(1/q))
def convolution_():
  vet = np.random.rand(16,1024)
  psy = np.random.rand(16)
  dist = np.array([calc_Lp(vet[tmp] , vet[tmp1]) for tmp in range(len(vet)) for tmp1 in range(len(vet)) if tmp != tmp1])
  plt.hist(dist , label="dist",bins=[0, 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])
    
  convol = np.array([np.convolve( psy , i) for i in vet])
 
  convol[convol >= 0.4*np.median(convol)] = 1
  convol[convol < 0.4*np.median(convol)] = 0
  dist_convol = np.array([calc_Lp(convol[i] , convol[j]) for i in range(len(convol)) for j in range(len(convol)) if i != j])
  plt.hist(dist_convol , label="dist_cov",bins=[0, 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])
  r = dist_convol*(0.4*np.median(convol))
  plt.hist(r,label="distribution")
  plt.legend()
  plt.xlabel("distance")
  plt.ylabel("Nombre")
  plt.show()


    
 
 
  

def main ():
  
   print(convolution_());
  

if __name__ == "__main__":
  main()

from sklearn.datasets import make_blobs
import numpy as np

def calc_Lq(x1 , x2 , q):
   A = np.array([x1,x2]);
   t2 = len(A.shape);
   i= 1;
   somme = 0;
   while i <= t2:
      somme = somme + abs(x1[i] - x2[i])**q;
      i = i +1;

   return (somme ** (1/q));
def update_assignation(X,U,q):
    n = len(X);
    k = len(U);
    assignations = [];
    dist = [];
    for i in range(n):
      for j in range(k):
        dist.append(calc_Lq(X[i],U[j],q));
        ind = np.argmin(dist);
      assignations[i](ind);
    return np.array(assignations)
    
 
def update_centroid(X,a,k):
    centroids = [];
    for i in range(len(X)):
      for idx en range(K):
        temp_cent = X[a[i] == idx].mean(axis=0)
        centroids = np.append(temp_cent)
      centroids = np.vstack(centroids)
    return centroids  
  

def main ():
   X1 ,y1 = make_blobs(n_samples=300, centers=2, n_features=2, random_state=0);
   X2, y2 = make_blobs(n_samples=300, centers=2, n_features=2, random_state=2);
   U = np.array([1,2,3,49]);
   print(calc_Lq(y1,y2,10));
   print(update_assignation(y1,y2,10));
   print(update_centroid(X,U,10));

if __name__ == "__main__":
  main()

#II. Manipulation

import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
im = cv.imread('texture3.tif', cv.IMREAD_GRAYSCALE)
msk_lap = np.array([[-1,0,2,0,-1],[-4,0,8,0,-4],[-6,0,12,0,-6],[-4,0,8,0,-4],[-1,0,2,0,-1]])
# Image convoluée 
convImage = cv.filter2D(im, -1 , msk_lap)
#plt.title("image convolué")
#plt.imshow(convImage,cmap="gray")

im1 = cv.imread('texture1.tif', cv.IMREAD_GRAYSCALE)
im2 = cv.imread('texture2.tif', cv.IMREAD_GRAYSCALE)
convImage1 = cv.filter2D(im1, -1 , msk_lap)
convImage2 = cv.filter2D(im2, -1 , msk_lap)
#plt.imshow(convImage,cmap="gray")
def variance_locale(img, mask):
    l, c = img.shape
    res = np.ones((l, c))
    m = np.floor(mask / 2.)
    m = int(m)
    for i in range(m, l-m):
        for j in range(m, c-m):
            a = img[i-m:i+m, j-m:j+m]
            v = a.var()
            res[i, j] = v
    return res 
mask = 5
result = variance_locale(convImage, mask)
attribut1 = variance_locale(convImage1, mask)
attribut2 = variance_locale(convImage2, mask)

valeur_moyenne1 = sum(attribut1)/(len(attribut1))
valeur_moyenne1 = sum(valeur_moyenne1)/(len(valeur_moyenne1))

valeur_moyenne2 = sum(attribut2)/(len(attribut2))
valeur_moyenne2 = sum(valeur_moyenne2)/(len(valeur_moyenne2))
#print("valeur moyenne de l'image attribut 1  :" + str(valeur_moyenne1))
#print("valeur moyenne de l'image attribut 2 :" + str(valeur_moyenne2))

#print(result)

#plt.hist(result.ravel(), 100)
#plt.title("result")
#plt.show() 

#3. Segmentation de l'image
imb = cv.threshold(im, 0 ,255 , cv.THRESH_BINARY | cv.THRESH_OTSU)
#print(imb)
#print(im)
#plt.imshow(imb)
#plt.imshow(imb, cmap=plt.cm.Greys)

gray = cv.cvtColor(im,cv.COLOR_BAYER_BG2BGR)
_,thresh = imb
edges = cv.dilate(cv.Canny(thresh,0,255),None)
#plt.title("Image texture3.tif segmenté")
#plt.imshow(edges,cmap="gray")
#Pour calculer la valeur moyenne et l'écart type de l'attribut


def gaussieneAttribut2(x):
  m = attribut2.mean()
  ec = np.sqrt(attribut2.var())
  res = (1/ (np.sqrt(2*np.pi) * ec)) * np.exp( (-(x-m)**2 ) / (2*ec**2) )
  return res
x = np.linspace(-3, 3, 120)
tmp = gaussieneAttribut2(x)
plt.plot(x,tmp)
plt.ylabel('gaussian distribution')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import cv2 as cv

im1 = cv.imread('ph_gauss.tif', cv.IMREAD_GRAYSCALE)

def median(I,fSize):      
  padding = (fSize-1)/2; #%padding size
  medArray = zeros(fSize,fSize);
  iPad = zeros(size(I) + padding*2); #% image with padding       
  #%copying original image to the new image with padding
  for x=1:size(I,1):
      for y=1:size(I,2):
          iPad(x+padding,y+padding) = I(x,y);
      end
  end
  
  newI = zeros(size(I));
  
#   %applying filter to the image
  for k=1:size(I,1)
      for l=1:size(I,2)              
          for x=1:fSize
              for y=1:fSize                
                  medArray(x,y) = iPad(k+x-1,l+y-1); %adding values to filter array              
              end
          end         
          sortedArray = sort(medArray(:)); %sorting filter array
          newI(k,l) = sortedArray((fSize^2+1)/2);  %getting median of the array    
      end
  end
  
  res = newI;
end

fSize = 3
median(im1,fSize)



# Commented out IPython magic to ensure Python compatibility.
import numpy as np

from sklearn.datasets import load_digits

from scipy.spatial.distance import pdist

from scipy import linalg

from sklearn.metrics import pairwise_distances

from scipy.spatial.distance import squareform

from sklearn.manifold import TSNE

from matplotlib import pyplot as plt

from sklearn.manifold._utils import _binary_search_perplexity
import seaborn as sns

sns.set(rc={'figure.figsize':(11.7,8.27)})

palette = sns.color_palette("bright", 10)

def joint_probabilities(distances, desired_perplexity, verbose):

    """Compute joint probabilities p_ij from distances.

    Parameters

    ----------

    distances : ndarray of shape (n_samples * (n_samples-1) / 2,)

        Distances of samples are stored as condensed matrices, i.e.

        we omit the diagonal and duplicate entries and store everything

        in a one-dimensional array.

    desired_perplexity : float

        Desired perplexity of the joint probability distributions.

    verbose : int

        Verbosity level.

    Returns

    -------

    P : ndarray of shape (n_samples * (n_samples-1) / 2,)

        Condensed joint probability matrix.

    """

    # Compute conditional probabilities such that they approximately match

    # the desired perplexity

    distances = distances.astype(np.float32, copy=False)
    

    conditional_P = _binary_search_perplexity(

        distances, desired_perplexity, verbose

    )
   

    P = conditional_P + conditional_P.T

    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)

    P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)

    return P

#Pour charger les données (N = 1797 et d = 64), utiliser la commande suivante : 



X, labels = load_digits(return_X_y=True)

MACHINE_EPSILON = np.finfo(np.double).eps

nc              = 2  #le nombre de dimensions à garder

perplexity      = 30

alpha           = max(nc - 1, 1)

def fit(X):
    n_samples = X.shape[0]
    
    # Compute euclidean distance
    distances = pairwise_distances(X, metric='euclidean', squared=True)
    print("Matrice D =" + str(distances))
    print("---------------------------------------------------------------")
    
    # Compute joint probabilities p_ij from distances.
    P = joint_probabilities(distances=distances, desired_perplexity=perplexity, verbose=False)
    print(P)
    
    # The embedding is initialized with iid samples from Gaussians with standard deviation 1e-4.
    X_embedded = 1e-4 * np.random.mtrand._rand.randn(n_samples, n_components).astype(np.float32)
    
    # degrees_of_freedom = n_components - 1 comes from
    # "Learning a Parametric Embedding by Preserving Local Structure"
    # Laurens van der Maaten, 2009.
    degrees_of_freedom = max(n_components - 1, 1)
    
    return _tsne(P, degrees_of_freedom, n_samples, X_embedded=X_embedded)

def _tsne(P, degrees_of_freedom, n_samples, X_embedded):
    params = X_embedded.ravel()
    
    obj_func = _kl_divergence
    
    params = _gradient_descent(obj_func, params, [P, degrees_of_freedom, n_samples, n_components])
        
    X_embedded = params.reshape(n_samples, n_components)
    return X_embedded

def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components):
    X_embedded = params.reshape(n_samples, n_components)
    
    dist = pdist(X_embedded, "sqeuclidean")
    dist /= degrees_of_freedom
    dist += 1.
    dist **= (degrees_of_freedom + 1.0) / -2.0
    print(dist)
    Q = np.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)
    
    # Kullback-Leibler divergence of P and Q
    kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))
    
    # Gradient: dC/dY
    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)
    PQd = squareform((P - Q) * dist)
    for i in range(n_samples):
        grad[i] = np.dot(np.ravel(PQd[i], order='K'),
                         X_embedded[i] - X_embedded)
    grad = grad.ravel()
    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom
    grad *= c
    return kl_divergence, grad

def _gradient_descent(obj_func, p0, args, it=0, n_iter=1000,
                      n_iter_check=1, n_iter_without_progress=300,
                      momentum=0.8, learning_rate=200.0, min_gain=0.01,
                      min_grad_norm=1e-7):
    
    p = p0.copy().ravel()
    update = np.zeros_like(p)
    gains = np.ones_like(p)
    error = np.finfo(np.float).max
    best_error = np.finfo(np.float).max
    best_iter = i = it
    
    for i in range(it, n_iter):
        error, grad = obj_func(p, *args)
        grad_norm = linalg.norm(grad)
        inc = update * grad < 0.0
        dec = np.invert(inc)
        gains[inc] += 0.2
        gains[dec] *= 0.8
        np.clip(gains, min_gain, np.inf, out=gains)
        grad *= gains
        update = momentum * update - learning_rate * grad
        p += update
        print("[t-SNE] Iteration %d: error = %.7f,"
                      " gradient norm = %.7f"
#                       % (i + 1, error, grad_norm))
        
        if error < best_error:
                best_error = error
                best_iter = i
        elif i - best_iter > n_iter_without_progress:
            break
        
        if grad_norm <= min_grad_norm:
            break
        return p

X_embedded = fit(X)

sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)

# Caractères 0 à 1023
for bloc in range(8):
   for lig in range(16):
      for col in range(8):
         code = 128 * bloc + 16 * col + lig
         caractere = chr(code)
         if code < 32:
            caractere = " "
         print("{:04d} {}  ".format(code, caractere), end = "")
      print()
   print()

# -*- coding: utf-8 -*-
 
def majuscsa(ch):
    """Convertit la chaine ch en minuscule non accentuées 
    """
    alpha1 = u"aàÀâÂäÄåÅæbcçÇdeéÉèÈêÊëËfghiîÎïÏjklmnoôÔöÖœpqrstuùÙûÛüÜvwxyÿŸz"
    alpha2 = u"AAAAAAAAAÆBCCCDEEEEEEEEEFGHIIIIIJKLMNOOOOOŒPQRSTUUUUUUUVWXYYYZ"
    x = ""
    for c in ch:
        k = alpha1.find(c) # k = indice de "c" dans alpha1
        if k >= 0:
            # "c" est dans alpha1: on remplace par le car. correspondant de alpha2
            x += alpha2[k]
        else:
            # "c" n'est pas dans alpha1: on le laisse passer
            x += c
    return x.lower()
 
ch = "éèçàùôï"
print(majuscsa(ch))

text = """> some_Varying_TEXT

DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF
GATACAACATAGGATACA
GGGGGAAAAAAAATTTTTTTTT
CCCCAAAA

> some_Varying_TEXT2

DJASDFHKJFHKSDHF
HHASGDFTERYTERE
GAGAGAGAGAG
PPPPPé
"""

import re

regex = re.compile(r'^>([^\n\r]+)[\n\r]([éA-Z\n\r]+)', re.MULTILINE)
matches = [m.groups() for m in regex.finditer(text)]

for m in matches:
    print('Name: %s\nSequence:%s' % (m[0], m[1].lower()))

import numpy as np
import matplotlib.pyplot as plt

def convol():
  vet = np.random.rand(16 , 1024)
  psy = np.random.rand(16)
  dist = np.array([np.linalg.norm(vet[tmp] - vet[tmp1]) for tmp in range(len(vet)) for tmp1 in range(len(vet)) if tmp != tmp1])
  plt.hist(dist , label="distance")
    
  convol = np.array([np.convolve(psy ,vet[i]) for i in range(len(vet)) ])
  seuil = 0.4*np.median(convol)
  convol[convol >= seuil] = 1
  convol[convol < seuil] = 0
  dist_convol = np.array([np.linalg.norm(convol[i] - convol[j]) for i in range(len(convol)) for j in range(len(convol)) if i != j])
  plt.hist(dist_convol , label="distance convol")
  plt.legend()
  plt.xlabel("value")
  plt.ylabel("nombre")
  plt.show()


    
 
 
  

def main ():
  
   print(convol());
  

if __name__ == "__main__":
  main()

